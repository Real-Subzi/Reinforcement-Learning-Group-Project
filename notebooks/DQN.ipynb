{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is DQN?\n",
    "So I asked my friend GPT to explain DQN to a person who only knows about linear approximators, and here is what I found.  \n",
    "\n",
    "Instead of hand-crafting features using tile coding or making them by ourselves, why not let the deep neural network learn them?  \n",
    "DQN (Deep Q-Network) approximates the Q-functions instead of storing Q-values like tabular Q-Learning.\n",
    "\n",
    "# Why go with DQN?\n",
    "Now we were struggling with making features using either tile coding or making the features ourselves. We do not have enough time before submission to make the features ourselves. Results and analysis are needed.   \n",
    "Therefore, we are going to use the help of DRL by using CNN (Convolutional Neural Network) for image processing and DQN to approximate whatever we get from CNN.  \n",
    "Also 2 of our team members took Machine Learning, so we are a bit familiar with CNN.  \n",
    "We are hoping for less training time so we can test on different alphas, gammas, and maybe rewards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets work on making the CNN\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import cv2\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are explanations for the numbers we are using for the CNN model:\n",
    "- Fitlers: \n",
    "    For filters we are going with 32, 64, 64 as it is a very common setup for CNN. We start 32 to detect edges and shapes. Then, going 64 fitlers twice to detect more complex shapes.\n",
    "- Kernel size:\n",
    "    For kernel size we start with a big enough size of 8x8 matrix that slides through the images. Calculation and operations are done to detect features. Then we decrease it to 4x4, and end with 3x3.\n",
    "- Strides:\n",
    "    Strides determine how far we move each time. We start with a big one 4, which aggressibely reduces spacial size. Then, 2 to further downsample it. Finally, we end with one stride "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "def create_cnn_model(input_shape, num_actions):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        # Rescaling is for normalizing the pixel values to be between 0 and 1\n",
    "        tf.keras.layers.Rescaling(1./255),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=8, strides=4, activation=\"relu\"),\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=4, strides=2, activation=\"relu\"),\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=3, strides=1, activation=\"relu\"),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "        # For DQN we dont need to use activation function\n",
    "        tf.keras.layers.Dense(num_actions)\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper classes and functions for DQN\n",
    "\n",
    "# Replay Buffer for DQN\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def ready(self, batch_size):\n",
    "        return len(self.buffer) >= batch_size\n",
    "    \n",
    "# Preprocess the image\n",
    "def preprocess(image):\n",
    "    image = cv2.resize(image, (96, 96))\n",
    "    return image\n",
    "\n",
    "# Train step\n",
    "@tf.function\n",
    "def train_step(model, target_model, optimizer, states, actions, rewards, next_states, dones, gamma):\n",
    "    future_q = tf.reduce_max(target_model(next_states), axis=1)\n",
    "    target_q = rewards + (1.0 - dones) * gamma * future_q\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = tf.reduce_sum(model(states) * tf.one_hot(actions, model.output_shape[-1]), axis=1)\n",
    "        loss = tf.keras.losses.MSE(target_q, q_values)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, input_shape, num_actions, alpha=0.1, gamma=0.9,\n",
    "                 epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.995,\n",
    "                 batch_size=64, buffer_size=100_000, target_update_freq=1000):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.steps = 0\n",
    "\n",
    "        self.policy_net = create_cnn_model(input_shape, num_actions)\n",
    "        self.target_net = create_cnn_model(input_shape, num_actions)\n",
    "        self.target_net.set_weights(self.policy_net.get_weights())\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=alpha)\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        state_input = np.expand_dims(state, axis=0)\n",
    "        q_values = self.policy_net.predict(state_input, verbose=0)\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    def store_experience(self, experience):\n",
    "        self.buffer.add(experience)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.set_weights(self.policy_net.get_weights())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def train(self):\n",
    "        if not self.buffer.ready(self.batch_size):\n",
    "            return\n",
    "        batch = self.buffer.sample(self.batch_size)\n",
    "        s, a, r, s2, d = map(np.array, zip(*batch))\n",
    "        train_step(\n",
    "            self.policy_net, self.target_net, self.optimizer,\n",
    "            tf.convert_to_tensor(s, dtype=tf.float32),\n",
    "            tf.convert_to_tensor(a, dtype=tf.int32),\n",
    "            tf.convert_to_tensor(r, dtype=tf.float32),\n",
    "            tf.convert_to_tensor(s2, dtype=tf.float32),\n",
    "            tf.convert_to_tensor(d, dtype=tf.float32),\n",
    "            self.gamma\n",
    "        )\n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "\n",
    "    def run_episode(self, env):\n",
    "        obs, _ = env.reset()\n",
    "        state = preprocess(obs)\n",
    "        done, total_reward = False, 0\n",
    "\n",
    "        while not done:\n",
    "            action = self.select_action(state)\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state = preprocess(next_obs)\n",
    "            self.store_experience((state, action, reward, next_state, float(done)))\n",
    "            self.train()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        self.decay_epsilon()\n",
    "        return total_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\user\\miniconda3\\envs\\cmps499\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Episode 1 finished with reward -56.810631229236726\n",
      "Episode 2 finished with reward -58.083832335330186\n",
      "Episode 3 finished with reward -51.492537313433665\n",
      "Episode 4 finished with reward -56.386292834891805\n",
      "Episode 5 finished with reward -60.91205211726474\n",
      "Episode 6 finished with reward -57.928802588997605\n",
      "Episode 7 finished with reward -54.703832752614026\n",
      "Episode 8 finished with reward -59.73154362416197\n",
      "Episode 9 finished with reward -53.020134228188766\n",
      "Episode 10 finished with reward -48.00000000000069\n",
      "Episode 11 finished with reward -58.199356913184076\n",
      "Episode 12 finished with reward -56.22895622895705\n",
      "Episode 13 finished with reward -54.24836601307264\n",
      "Episode 14 finished with reward -61.937716262976636\n",
      "Episode 15 finished with reward -52.7272727272735\n",
      "Episode 16 finished with reward -52.21843003413058\n",
      "Episode 17 finished with reward -54.83870967742016\n",
      "Episode 18 finished with reward -61.90476190476276\n",
      "Episode 19 finished with reward -49.806949806950634\n",
      "Episode 20 finished with reward -52.02952029520377\n",
      "Episode 21 finished with reward -57.05521472392723\n",
      "Episode 22 finished with reward -51.388888888889696\n",
      "Episode 23 finished with reward -54.861111111111974\n",
      "Episode 24 finished with reward -59.62732919254741\n",
      "Episode 25 finished with reward -52.38095238095314\n",
      "Episode 26 finished with reward -52.702702702703476\n",
      "Episode 27 finished with reward -68.50393700787446\n",
      "Episode 28 finished with reward -51.768488745981486\n",
      "Episode 29 finished with reward -51.38888888888954\n",
      "Episode 30 finished with reward -56.81063122923671\n",
      "Episode 31 finished with reward -59.86622073578678\n",
      "Episode 32 finished with reward -53.75722543352686\n",
      "Episode 33 finished with reward -49.45848375451338\n",
      "Episode 34 finished with reward -50.53003533568982\n",
      "Episode 35 finished with reward -57.295373665481186\n",
      "Episode 36 finished with reward -55.631399317406974\n",
      "Episode 37 finished with reward -61.53846153846241\n",
      "Episode 38 finished with reward -59.322033898306024\n",
      "Episode 39 finished with reward -55.71955719557279\n",
      "Episode 40 finished with reward -54.37262357414523\n",
      "Episode 41 finished with reward -55.22388059701572\n",
      "Episode 42 finished with reward -57.29537366548123\n",
      "Episode 43 finished with reward -56.79012345679103\n",
      "Episode 44 finished with reward -52.21843003413044\n",
      "Episode 45 finished with reward -54.8192771084346\n",
      "Episode 46 finished with reward -52.70270270270329\n",
      "Episode 47 finished with reward -41.86046511627975\n",
      "Episode 48 finished with reward -55.22388059701583\n",
      "Episode 49 finished with reward -54.703832752614005\n",
      "Episode 50 finished with reward -51.298701298702\n",
      "Episode 51 finished with reward -58.06451612903315\n",
      "Episode 52 finished with reward -54.54545454545531\n",
      "Episode 53 finished with reward -56.081081081081905\n",
      "Episode 54 finished with reward -52.3809523809532\n",
      "Episode 55 finished with reward -60.91205211726462\n",
      "Episode 56 finished with reward -47.91666666666742\n",
      "Episode 57 finished with reward -56.81063122923666\n",
      "Episode 58 finished with reward -58.990536277603354\n",
      "Episode 59 finished with reward -57.79220779220864\n",
      "Episode 60 finished with reward -54.86111111111197\n",
      "Episode 61 finished with reward -53.17725752508446\n",
      "Episode 62 finished with reward -55.326460481100476\n",
      "Episode 63 finished with reward -54.098360655738524\n",
      "Episode 64 finished with reward -50.87719298245689\n",
      "Episode 65 finished with reward -52.94117647058901\n",
      "Episode 66 finished with reward -51.92307692307778\n",
      "Episode 67 finished with reward -53.7953795379546\n",
      "Episode 68 finished with reward -54.24836601307277\n",
      "Episode 69 finished with reward -48.616600790514475\n",
      "Episode 70 finished with reward -58.62068965517329\n",
      "Episode 71 finished with reward -49.45848375451331\n",
      "Episode 72 finished with reward -49.01960784313801\n",
      "Episode 73 finished with reward -49.152542372882216\n",
      "Episode 74 finished with reward -54.703832752614105\n",
      "Episode 75 finished with reward -54.86111111111183\n",
      "Episode 76 finished with reward -48.71794871794942\n",
      "Episode 77 finished with reward -49.45848375451335\n",
      "Episode 78 finished with reward -42.622950819672624\n",
      "Episode 79 finished with reward -53.90070921985897\n",
      "Episode 80 finished with reward -52.8619528619536\n",
      "Episode 81 finished with reward -49.640287769784955\n",
      "Episode 82 finished with reward -43.262411347518494\n",
      "Episode 83 finished with reward -40.350877192982956\n",
      "Episode 84 finished with reward -57.654723127036654\n",
      "Episode 85 finished with reward -57.51633986928187\n",
      "Episode 86 finished with reward -55.27156549520851\n",
      "Episode 87 finished with reward -50.877192982456876\n",
      "Episode 88 finished with reward -55.47945205479534\n",
      "Episode 89 finished with reward -45.94594594594654\n",
      "Episode 90 finished with reward -52.861952861953654\n",
      "Episode 91 finished with reward -48.553054662380255\n",
      "Episode 92 finished with reward -33.33333333333388\n",
      "Episode 93 finished with reward -46.043165467626615\n",
      "Episode 94 finished with reward -55.83596214511126\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    num_episodes = 300\n",
    "    # Define the environment\n",
    "    env = gym.make(\"CarRacing-v3\", continuous=False)\n",
    "    action_space = env.action_space.n\n",
    "    state_space = env.observation_space.shape\n",
    "    agent = DQNAgent(state_space, action_space)\n",
    "\n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        total_reward = agent.run_episode(env)\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1} finished with reward {total_reward}\")\n",
    "\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version: 2.15.0\n",
      "Built with CUDA: False\n",
      "GPUs: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TF Version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmps499",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
