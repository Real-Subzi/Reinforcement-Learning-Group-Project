{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to the project purpose\n",
    "In this project we are going to `conduct a reinforcement learning (RL) project comparing tabular and function approximation methods covered up to Chapter 10`.   \n",
    "\n",
    "## We are going to explore:\n",
    "- Exploration strategies.\n",
    "- Learning rate impacts.\n",
    "- Reward structures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to the Environment\n",
    "In this project we are covering the Car Racing environment from [Gymnasium](https://gymnasium.farama.org/environments/box2d/car_racing/)   \n",
    "## Box2D\n",
    "Before explaining the environment, we need to know about Box2D.   \n",
    "[Box2D](https://github.com/erincatto/box2d) is physics engine for making 2D games.   \n",
    "It offers the capability to make the world interactive and as real to the real world physics [Box2D docs](https://box2d.org/documentation/)\n",
    "Car Racing uses Box2D as the engine for it.\n",
    "## Car Racing\n",
    ">Note: please notice that this section is summarized from the [Gymnasium Car Racing documentation](https://gymnasium.farama.org/environments/box2d/car_racing/)\n",
    "\n",
    "Car Racing is a `top-down racing environemnt` made from pixels that generates random tracks every episode.   \n",
    "The goal is for the car to pass the whole racing track without failure as efficiently as possible.   \n",
    "The environment could generate and RGB buffer to view the environment world along with indicators that show the `true speed, ABS sensors, steering wheel position, and gyroscope`. But those indicators are only shown for viewing purposes, it is not used as a reward system or an observation state. The observation state is solely the pixels of the environemnt.   \n",
    "### Actions Space\n",
    "Car Racing offers 2 types of actions: continuous, and discrete.\n",
    "#### Continuous\n",
    "- 0: steering [-1,1] -1 for left, 1 for light\n",
    "- 1: gas [0,1]\n",
    "- 2: braking [0,1]\n",
    "#### Discrete\n",
    "- 0: do nothing\n",
    "- 1: left\n",
    "- 2: right\n",
    "- 3: gas\n",
    "- 4: brake   \n",
    "We are going to cover discrete action only, as it is simpler and we cannot use continuous action space for tabular methods.\n",
    "### Observation Space\n",
    "The observation space represents the POV of the environment as a top-down camera.\n",
    "It is an image of the car and the race track.\n",
    "The space is **96x96 RGB image**. Therefore giving the size of 96x96x3=27,648.\n",
    "### Reward Structure\n",
    "The normal reward structure taken from the gym step functions gives the following:\n",
    "- -0.1: for each frame.\n",
    "- $ 1000 \\over N $ for every tile visited. (N=total number of tiles visited in the track)\n",
    "- -100 when the car dies. (car death explained below)\n",
    "### Episode Termination\n",
    "The episode terminates when either:\n",
    "- All tiles are visited.\n",
    "- Car went out of bounds of the playfield, and therefore it dies.\n",
    "### Environment Arguments\n",
    "- `lap_complete_percent=0.95`: to change the amound of tiles to be visited to consider the lap as completed.\n",
    "- `domain_randomize=False`: to randomize the colors of the track and the field.\n",
    "- `continuous=True`: to use continuous or discrete action space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem of large state size\n",
    "Through initial research (with ChatGPT) and discussions with our instructor, we have found that Car Racing only produces pixels as an observation space, and this is huge `27,648` for tabular and function approximation methods. Therefore we need to handle this case before proceeding with the project. We must find a way to reduce the state size or handle with it differently.\n",
    "## A blog for dealing with Car Racing\n",
    "We have walked through a blog made by an Engineer who refers to himself as [Mike](https://notanymike.github.io/Solving-CarRacing/).\n",
    "The blog covers how to train the Car Racing model with Proximal Policy Optimisations (PPO), which looks like a deep RL method but nevertheless, his discoveries are inightful for our case.\n",
    "### Discrete actions Case \n",
    "Car Racing offers a discrete action space which makes the agent only perform one actions at a time (do nothing, turn left, turn right, gas, brake) which provides simplicity but produces a new problem when dealing with track curves.\n",
    "In a real scenario, the car brakes AND turns as a one action. This might effect the performance for our case if we did not handle it.\n",
    "The author handled the case by providing soft discrete actions. It provides more actions that perform hard or soft turning which gives full or partial values for turning and acceleraing, or turning and braking.\n",
    "### Simplifying the observation space\n",
    "The author simplified the observation space by:\n",
    "1. Removing the bottom panel that shows observable stats for the human observer. Since this is useless for the agent, removing it is better to avoid wasteful training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmps460",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
