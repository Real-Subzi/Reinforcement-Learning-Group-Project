{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to the project purpose\n",
    "In this project we are going to `conduct a reinforcement learning (RL) project comparing tabular and function approximation methods covered up to Chapter 10`.   \n",
    "\n",
    "## We are going to explore:\n",
    "- Exploration strategies.\n",
    "- Learning rate impacts.\n",
    "- Reward structures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to the Environment\n",
    "In this project we are covering the Car Racing environment from [Gymnasium](https://gymnasium.farama.org/environments/box2d/car_racing/)   \n",
    "## Box2D\n",
    "Before explaining the environment, we need to know about Box2D.   \n",
    "[Box2D](https://github.com/erincatto/box2d) is physics engine for making 2D games.   \n",
    "It offers the capability to make the world interactive and as real to the real world physics [Box2D docs](https://box2d.org/documentation/)\n",
    "Car Racing uses Box2D as the engine for it.\n",
    "## Car Racing\n",
    ">Note: please notice that this section is summarized from the [Gymnasium Car Racing documentation](https://gymnasium.farama.org/environments/box2d/car_racing/)\n",
    "\n",
    "Car Racing is a `top-down racing environemnt` made from pixels that generates random tracks every episode.   \n",
    "The goal is for the car to pass the whole racing track without failure as efficiently as possible.   \n",
    "The environment could generate and RGB buffer to view the environment world along with indicators that show the `true speed, ABS sensors, steering wheel position, and gyroscope`. But those indicators are only shown for viewing purposes, it is not used as a reward system or an observation state. The observation state is solely the pixels of the environemnt.   \n",
    "### Actions Space\n",
    "Car Racing offers 2 types of actions: continuous, and discrete.\n",
    "#### Continuous\n",
    "- 0: steering [-1,1] -1 for left, 1 for light\n",
    "- 1: gas [0,1]\n",
    "- 2: braking [0,1]\n",
    "#### Discrete\n",
    "- 0: do nothing\n",
    "- 1: left\n",
    "- 2: right\n",
    "- 3: gas\n",
    "- 4: brake\n",
    "We are going to cover discrete action only, as it is simpler and we cannot use continuous action space for tabular methods.\n",
    "### Observation Space\n",
    "The observation space represents the POV of the environment as a top-down camera.\n",
    "It is an image of the car and the race track.\n",
    "The space is **96x96 RGB image**. Therefore giving the size of 96x96x3=27,648.\n",
    "### Reward Structure\n",
    "The normal reward structure taken from the gym step functions gives the following:\n",
    "- -0.1: for each frame.\n",
    "- $ 1000 \\over N $ for every tile visited. (N=total number of tiles visited in the track)\n",
    "- -100 when the car dies. (car death explained below)\n",
    "### Episode Termination\n",
    "The episode terminates when either:\n",
    "- All tiles are visited.\n",
    "- Car went out of bounds of the playfield, and therefore it dies.\n",
    "### Environment Arguments\n",
    "- `lap_complete_percent=0.95`: to change the amound of tiles to be visited to consider the lap as completed.\n",
    "- `domain_randomize=False`: to randomize the colors of the track and the field.\n",
    "- `continuous=True`: to use continuous or discrete action space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmps460",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
