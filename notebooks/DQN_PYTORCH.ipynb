{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with reward -83.99999999999946\n",
      "Episode 2 finished with reward -83.99999999999946\n",
      "Episode 3 finished with reward -83.99999999999937\n",
      "Episode 4 finished with reward -82.89999999999947\n",
      "Episode 5 finished with reward -81.79999999999956\n",
      "Episode 6 finished with reward -81.79999999999953\n",
      "Episode 7 finished with reward -83.99999999999946\n",
      "Episode 8 finished with reward -82.8999999999995\n",
      "Episode 9 finished with reward -83.99999999999949\n",
      "Episode 10 finished with reward -81.79999999999961\n",
      "Episode 11 finished with reward -82.8999999999995\n",
      "Episode 12 finished with reward -85.0999999999994\n",
      "Episode 13 finished with reward -82.89999999999947\n",
      "Episode 14 finished with reward -82.89999999999955\n",
      "Episode 15 finished with reward -82.89999999999944\n",
      "Episode 16 finished with reward -81.79999999999964\n",
      "Episode 17 finished with reward -80.69999999999962\n",
      "Episode 18 finished with reward -82.89999999999947\n",
      "Episode 19 finished with reward -82.89999999999955\n",
      "Episode 20 finished with reward -79.59999999999977\n",
      "Episode 21 finished with reward -81.79999999999953\n",
      "Episode 22 finished with reward -82.89999999999947\n",
      "Episode 23 finished with reward -79.59999999999968\n",
      "Episode 24 finished with reward -81.79999999999953\n",
      "Episode 25 finished with reward -81.79999999999953\n",
      "Episode 26 finished with reward -81.79999999999961\n",
      "Episode 27 finished with reward -82.89999999999955\n",
      "Episode 28 finished with reward -82.89999999999947\n",
      "Episode 29 finished with reward -83.9999999999994\n",
      "Episode 30 finished with reward -82.89999999999955\n",
      "Episode 31 finished with reward -80.69999999999968\n",
      "Episode 32 finished with reward -81.79999999999953\n",
      "Episode 33 finished with reward -82.89999999999947\n",
      "Episode 34 finished with reward -85.0999999999994\n",
      "Episode 35 finished with reward -81.79999999999953\n",
      "Episode 36 finished with reward -82.89999999999955\n",
      "Episode 37 finished with reward -82.89999999999947\n",
      "Episode 38 finished with reward -83.99999999999946\n",
      "Episode 39 finished with reward -82.89999999999955\n",
      "Episode 40 finished with reward -82.89999999999947\n",
      "Episode 41 finished with reward -83.99999999999946\n",
      "Episode 42 finished with reward -82.89999999999947\n",
      "Episode 43 finished with reward -81.79999999999956\n",
      "Episode 44 finished with reward -80.69999999999959\n",
      "Episode 45 finished with reward -82.89999999999955\n",
      "Episode 46 finished with reward -83.99999999999946\n",
      "Episode 47 finished with reward -86.19999999999928\n",
      "Episode 48 finished with reward -81.79999999999961\n",
      "Episode 49 finished with reward -85.09999999999937\n",
      "Episode 50 finished with reward -82.89999999999961\n",
      "Saved model weights to torch_weights/dqn_policy_weights_ep50.pth\n",
      "Episode 51 finished with reward -85.0999999999994\n",
      "Episode 52 finished with reward -82.89999999999947\n",
      "Episode 53 finished with reward -86.19999999999933\n",
      "Episode 54 finished with reward -83.99999999999949\n",
      "Episode 55 finished with reward -83.9999999999994\n",
      "Episode 56 finished with reward -83.9999999999994\n",
      "Episode 57 finished with reward -83.9999999999994\n",
      "Episode 58 finished with reward -85.0999999999994\n",
      "Episode 59 finished with reward -81.79999999999959\n",
      "Episode 60 finished with reward -80.69999999999968\n",
      "Episode 61 finished with reward -85.0999999999994\n",
      "Episode 62 finished with reward -82.89999999999944\n",
      "Episode 63 finished with reward -83.99999999999946\n",
      "Episode 64 finished with reward -85.0999999999994\n",
      "Episode 65 finished with reward -82.89999999999952\n",
      "Episode 66 finished with reward -82.8999999999995\n",
      "Episode 67 finished with reward -83.99999999999955\n",
      "Episode 68 finished with reward -82.89999999999947\n",
      "Episode 69 finished with reward -82.89999999999944\n",
      "Episode 70 finished with reward -81.79999999999964\n",
      "Episode 71 finished with reward -83.9999999999994\n",
      "Episode 72 finished with reward -82.89999999999955\n",
      "Episode 73 finished with reward -81.79999999999967\n",
      "Episode 74 finished with reward -83.99999999999949\n",
      "Episode 75 finished with reward -83.99999999999946\n",
      "Episode 76 finished with reward -82.89999999999941\n",
      "Episode 77 finished with reward -79.59999999999968\n",
      "Episode 78 finished with reward -83.9999999999994\n",
      "Episode 79 finished with reward -83.9999999999994\n",
      "Episode 80 finished with reward -83.9999999999994\n",
      "Episode 81 finished with reward -81.79999999999961\n",
      "Episode 82 finished with reward -81.79999999999956\n",
      "Episode 83 finished with reward -81.79999999999964\n",
      "Episode 84 finished with reward -83.9999999999994\n",
      "Episode 85 finished with reward -83.99999999999937\n",
      "Episode 86 finished with reward -82.89999999999955\n",
      "Episode 87 finished with reward -83.9999999999994\n",
      "Episode 88 finished with reward -81.79999999999961\n",
      "Episode 89 finished with reward -80.6999999999997\n",
      "Episode 90 finished with reward -82.89999999999955\n",
      "Episode 91 finished with reward -79.59999999999968\n",
      "Episode 92 finished with reward -81.79999999999961\n",
      "Episode 93 finished with reward -83.99999999999946\n",
      "Episode 94 finished with reward -83.99999999999946\n",
      "Episode 95 finished with reward -83.9999999999994\n"
     ]
    }
   ],
   "source": [
    "# DQN_PYTORCH.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import cv2\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "IMG_SIZE = 96\n",
    "FRAME_STACK = 1  # You can increase this if you want to stack frames\n",
    "BUFFER_SIZE = 100_000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 0.995\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "CROP_HEIGHT_PERCENTAGE = 0.12\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def preprocess(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "# --- Skip Frames ---\n",
    "def skip_frames(env, skip=50):\n",
    "    for _ in range(skip):\n",
    "        action = env.action_space.sample()\n",
    "        obs, _, _, _, _ = env.step(action)\n",
    "    return obs\n",
    "\n",
    "# --- Replay Buffer ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s, a, r, s2, d = map(np.array, zip(*batch))\n",
    "        return s, a, r, s2, d\n",
    "\n",
    "    def ready(self, batch_size):\n",
    "        return len(self.buffer) >= batch_size\n",
    "\n",
    "# --- CNN Model ---\n",
    "class DQNCNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super().__init__()\n",
    "        c, h, w = input_shape\n",
    "        # Define conv layers first\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU(),\n",
    "        )\n",
    "        # Compute conv output size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, c, h, w)\n",
    "            conv_out_size = self.conv(dummy).view(1, -1).size(1)\n",
    "        # Now define the full network\n",
    "        self.net = nn.Sequential(\n",
    "            self.conv,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_out_size, 512), nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- DQN Agent ---\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_shape, num_actions, alpha=ALPHA, gamma=GAMMA,\n",
    "                 epsilon_start=EPSILON_START, epsilon_end=EPSILON_END, epsilon_decay=EPSILON_DECAY,\n",
    "                 batch_size=BATCH_SIZE, buffer_size=BUFFER_SIZE, target_update_freq=TARGET_UPDATE_FREQ):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.steps = 0\n",
    "\n",
    "        self.policy_net = DQNCNN(input_shape, num_actions).to(DEVICE)\n",
    "        self.target_net = DQNCNN(input_shape, num_actions).to(DEVICE)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=alpha)\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)  # NHWC -> NCHW\n",
    "        return q_values.argmax(1).item()\n",
    "\n",
    "    def store_experience(self, experience):\n",
    "        self.buffer.add(experience)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def train(self):\n",
    "        if not self.buffer.ready(self.batch_size):\n",
    "            return\n",
    "        s, a, r, s2, d = self.buffer.sample(self.batch_size)\n",
    "        # Convert to tensors\n",
    "        states = torch.tensor(s, dtype=torch.float32, device=DEVICE)\n",
    "        next_states = torch.tensor(s2, dtype=torch.float32, device=DEVICE)\n",
    "        if states.ndim == 3:\n",
    "            states = states.unsqueeze(1)   \n",
    "            next_states = next_states.unsqueeze(1)\n",
    "        actions = torch.tensor(a, dtype=torch.int64, device=DEVICE)\n",
    "        rewards = torch.tensor(r, dtype=torch.float32, device=DEVICE)\n",
    "        dones = torch.tensor(d, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        # Q(s, a)\n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        # max_a' Q_target(s', a')\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1)[0]\n",
    "        targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "\n",
    "    def run_episode(self, env):\n",
    "        obs, _ = env.reset()\n",
    "        obs = skip_frames(env)\n",
    "        state = preprocess(obs)\n",
    "        done, total_reward = False, 0\n",
    "\n",
    "        while not done:\n",
    "            action = self.select_action(state)\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            # modifying the reward function to not get exited when getting huge\n",
    "            reward = np.clip(reward,a_min=np.NINF, a_max=1.0)\n",
    "            done = terminated or truncated\n",
    "            next_state = preprocess(next_obs)\n",
    "            self.store_experience((state, action, reward, next_state, float(done)))\n",
    "            self.train()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        self.decay_epsilon()\n",
    "        return total_reward\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "def main():\n",
    "    num_episodes = 300\n",
    "    env = gym.make(\"CarRacing-v3\", continuous=False)\n",
    "    action_space = env.action_space.n\n",
    "    state_space = (1, IMG_SIZE, IMG_SIZE)\n",
    "    agent = DQNAgent(state_space, action_space)\n",
    "\n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        total_reward = agent.run_episode(env)\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1} finished with reward {total_reward}\")\n",
    "\n",
    "        # Save every 50 episodes\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            checkpoint_path = f\"torch_weights/dqn_policy_weights_ep{episode+1}.pth\"\n",
    "            torch.save(agent.policy_net.state_dict(), checkpoint_path)\n",
    "            print(f\"Saved model weights to {checkpoint_path}\")\n",
    "\n",
    "    checkpoint_path = f\"torch_weights/dqn_policy_weights_final.pth\"\n",
    "    torch.save(agent.policy_net.state_dict(), checkpoint_path)\n",
    "    print(f\"Saved model weights to {checkpoint_path}\")\n",
    "\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"DQN Training Performance\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
